\documentclass[../../main/main.tex]{subfiles}

\begin{document}

\section{Artificial Intelligence and Machine Learning for nuclear physics in broad strokes}


 Statistics, data science, and AI/ML form important fields of research
 in modern science. They describe how to learn and make predictions
 from data, and enable the extraction of key information about
 physical processes and the underlying scientific laws based on large
 datasets. As such, recent advances in AI capabilities are being
 applied to advance scientific discoveries in the physical sciences.




Ideally, AI represents the science of building models to perform a
task without being explicitly programmed. ML tasks fall under the
broader AI umbrella. We will henceforth refer to the methods discussed
as ``AI/ML''. The idea is that there exist generic algorithms which
can be used to find patterns in a broad class of datasets without
having to write code specifically for each problem. The algorithm
builds its own logic based on the data. The attentive reader should
however always keep in mind that machines and algorithms are to a
large extent developed by humans. The choice of a specific AI/ML
algorithm is governed by the insights and knowledge about a specific
system.



There exist many AI/ML approaches; they are often split into two main
categories, supervised and unsupervised. In supervised learning, data
are labeled and one lets a specific ML algorithm learn and deduce
patterns in the datasets. This allows one to make predictions about
future events and/or data not included in the training set.  On the
other hand, unsupervised learning is a method for finding patterns and
relationship in datasets without any prior knowledge of the
system. Many researchers also operate with a third category, dubbed
reinforcement learning. This is a paradigm of learning inspired by
behavioral psychology, where actions are learned to maximize reward.
One may encounter reinforcement learning being accompanied by
supervised deep learning methods. Furthermore, what is often referred
to as semi-supervised learning, entails developing algorithms that aim
at learning from a dataset that includes both labeled and unlabeled
data.


Another way to categorize AI/ML tasks is to consider the desired output of a system. Some of the most common tasks are:
\makeatletter
\renewenvironment{description}%
               {\list{}{\leftmargin=10pt % <------- Adjust this length
                        \labelwidth\z@ \itemindent-\leftmargin
                        \let\makelabel\descriptionlabel}}%
               {\endlist}
\makeatother
\begin{description}
    \item[Classification] Outputs are divided into two or more classes. The goal is to produce a model that assigns inputs into one of these classes. An example is to identify digits based on pictures of hand-written numbers. 
\item[Regression] Finding a functional relationship between an input dataset and a reference dataset. The goal is to construct a function that maps input data into continuous output values.
\item[Clustering] Data are divided into groups with certain common traits, without knowing the different groups beforehand. This AI/ML task falls  under the category of unsupervised learning.
\item[Generation] Building a model to generate data that are akin to a training dataset in both examples and distributions of examples. Most generative models are types of unsupervised learning.
\end{description}
In Table\,\ref{tab:acronyms} we list many of the methods encountered in this work, with their respective abbreviations.
\begin{table*}[!htb]
 \caption{Table of AI/ML with indication on the main type of learning (S: supervised,  U: unsupervised,  Semi-S: semi-supervised).
 }
    \label{tab:acronyms}
\begin{ruledtabular}
    \begin{tabular}{cll} 
    Acronym & Method & Type of Learning \\ \hline 
    AE & Autoencoders & U \\
    ANN     & Artificial Neural Networks & S  \\ 
  BED & Bayesian Experimental Design   & S \\
        BM & Boltzmann Machines & U \\
    BMA & Bayesian Model Averaging & S \\
    BMM & Bayesian Model Mixing & Semi-S \\
    BO & Bayesian Optimization & S \\
    BNN & Bayesian Neural Networks & S \\
%  CL & Clustering Methods & U \\ %MPK none found in text 
    CNN & Convolutional Neural Networks & S \\
     EMB  & Ensemble Methods  and Boosting, including Decision Trees and Random Forests & S \\
  %  FFNN & Feed Forward Neural Network & S \\  %MPK changed in text
    GAN & Generative Adversarial Networks  & U \\
    GP & Gaussian Processes  & Semi-S \\
    KNN & $k$-nearest neighbors & U \\
    KR & Kernel Regression & S \\
    LR & Logistic Regression & S\\ 
    LSTM & Long short-term memory & S\\
%    MLP & Multilayer Perceptron & S\\ %MPK changed in text
    PCA & Principal Component Analysis \& Dimensionality Reduction & U \\
    REG    & Linear Regression  & S \\
    RL & Reinforcement Learning & Neither S nor U \\
    RNN & Recurrent Neural Networks & S \\
    SVM & Support Vector Machines & S \\
    VAE & Variational Auto Encoders & U 
     \end{tabular}
 \end{ruledtabular}  
\end{table*}


The methods we cover here have three central elements in common,
irrespective of whether we deal with supervised, unsupervised, or
semi-supervised learning. The first element is some dataset (which can
be subdivided into training, validation, and test data), the second
element is a model, which is normally a function of some parameters to
be determined by the chosen optimization process. The model reflects
our prior knowledge of the system (or lack thereof). As an example, if
we know that our data show a behavior similar to what would be
predicted by a polynomial, fitting the data to a polynomial of some
degree would determine our model.  The last element is a so-called
cost (or loss, error, penalty, or risk) function which allows us to
present an estimate on how good our model is in reproducing the data
it is supposed to train. This is the function which is optimized in
order to obtain the best prediction for the data under study. The
simplest cost function in a regression analysis (fitting a continuous
function to the data) is the so-called mean squared error function
while for a binary classification problem, the so-called cross entropy
is widely used, see, e.g., \cite{Murphy2012,Bishop2006,Hastie2009} for
more details. We will henceforth refer to this element as the
assessment of a given method.


 
Traditionally, the field of AI/ML has had its main focus on
predictions and correlations.  In AI/ML and prediction-based tasks, we
are often interested in developing algorithms that are capable of
learning patterns from existing data in an automated fashion, and then
using these learned patterns to make predictions or assessments of new
data. In some cases, our primary concern is the quality of the
predictions or assessments, with perhaps less focus on the underlying
patterns (and probability distributions) that were learned in order to
make these predictions.  However, in many nuclear physics studies, we
are equally interested in being able to estimate errors and find
causations.  In this Colloquium, we emphasize the role of predictions
and correlations as well as error estimation and causations in
statistical learning and ML.  For general references on these topics
and discussions of frequentist and Bayesian methodologies, see, e.g.,
\cite{Bishop2006, Goodfellow2016, Murphy2012, Hastie2009}.

\end{document}